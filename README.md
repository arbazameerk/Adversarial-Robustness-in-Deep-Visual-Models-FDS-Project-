# Adversarial-Robustness-in-Deep-Visual-Models-FDS-Project-
FGSM/PGD attacks &amp; adversarial training on ResNet-18 (CIFAR-10) – 99% ASR, robust defense


# Project Description

This repository contains the complete codebase, experiments, report, and video demo for the **CS685/785 Group Project on Adversarial Machine Learning**.

We investigate the vulnerability of Convolutional Neural Networks (CNNs) to adversarial examples and explore methods to improve their robustness.  
Our team implemented and evaluated state-of-the-art adversarial attack techniques (FGSM, PGD) as well as adversarial training as a defense mechanism on the CIFAR-10 dataset using a ResNet-18 architecture.

The project demonstrates:
- High attack success rates with imperceptible perturbations
- Trade-offs between clean accuracy and robust accuracy
- Effectiveness of adversarial training under varying attack strengths (ε values and iteration counts)

# Contributors

| Name            | Role / Primary Contribution                                      |
|-----------------|------------------------------------------------------------------|
| Arbaz Ameer     | Project Lead • Overall architecture • Report writing             |
| Mahi Patel      | Model training pipeline • Hyperparameter tuning • Visualizations |
| Tanmay Joshi    | Adversarial attack implementation (FGSM, PGD) • ASR evaluation  |
| Manav Seth      | Adversarial training • Defense implementation • Robustness testing |
| Krushil Patel   | Dataset preprocessing • Experiment logging • Video demo & plots  |

All team members actively contributed to debugging, code reviews, result analysis, report finalization, and the video presentation.
